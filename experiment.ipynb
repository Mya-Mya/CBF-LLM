{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "from torch import Tensor, no_grad, hstack\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Transformers\n",
    "import transformers\n",
    "from transformers import TextStreamer\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "# Other Modules\n",
    "from importlib import reload\n",
    "from typing import List, Union\n",
    "import time\n",
    "from functools import cache\n",
    "\n",
    "# Reloading CBF-LLM Modules\n",
    "import torch_utils\n",
    "import language_constraint_functions\n",
    "import filter\n",
    "import cbf_filter\n",
    "import blacklist_filter\n",
    "import just_topk_filter\n",
    "import normalizers\n",
    "import token_predictors\n",
    "for module in [torch_utils, language_constraint_functions, filter, cbf_filter, blacklist_filter, just_topk_filter, normalizers, token_predictors]:\n",
    "    reload(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBF-LLM Classes\n",
    "from just_topk_filter import JustTopkFilter\n",
    "from blacklist_filter import BlacklistFilter\n",
    "from cbf_filter import CBFFilter\n",
    "from filter import Filter, FilterResult\n",
    "from language_constraint_functions import LanguageCF\n",
    "from normalizers import Normalizer, MinJSDNormalizer, Min2NormNormalizer\n",
    "from torch_utils import *\n",
    "from token_predictors import distributionify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行時間の計測\n",
    "_tic_time = time.time()\n",
    "def tic():\n",
    "    global _tic_time\n",
    "    _tic_time = time.time()\n",
    "def toc(print_time: bool = True) -> Union[None, float]:\n",
    "    t = time.time() - _tic_time\n",
    "    if print_time:\n",
    "        print(f\"{t:.04f} 秒\")\n",
    "    else:\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "制約言語関数$h:\\mathcal X \\to \\mathbb R$を作る．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "# https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# 当該モデルをダウンロードし，ダウンロード先のパスを指定してください．\n",
    "name = input(\"Name for cardiffnlp/twitter-roberta-base-sentiment-latest?\")\n",
    "\n",
    "\n",
    "def _mapper(output: SequenceClassifierOutput) -> List[float]:\n",
    "    \"\"\"\n",
    "    元の感情推定RoBERTaモデルからhの値を計算する．\n",
    "    \"\"\"\n",
    "    scores = torch.softmax(output[0], dim=1)\n",
    "    negatives, neutrals, positives = scores.T\n",
    "    h_list = positives - torch.max(negatives, neutrals)\n",
    "    return tolist(h_list)\n",
    "\n",
    "\n",
    "lcf = LanguageCF(\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(name),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(name),\n",
    "    mapper=_mapper,\n",
    "    name=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMを起動する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace meta-llama/Meta-Llama-3-8B\n",
    "# https://huggingface.co/meta-llama/Meta-Llama-3-8B\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "name = input(\"Name for meta-llama/Meta-Llama-3-8B?\")\n",
    "\n",
    "Gm = LlamaForCausalLM.from_pretrained(name, torch_dtype=torch.float16).to(device)\n",
    "Gt = AutoTokenizer.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(Gt)\n",
    "vocab = Gt.get_vocab()\n",
    "ivocab = {v: Gt.decode([v]).replace(\"\\n\", \"\") for k, v in vocab.items()}\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "\n",
    "@cache\n",
    "def Gtokenize(xstr: str) -> Tensor:\n",
    "    Ginputs = Gt(xstr, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "    x = Ginputs.input_ids[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\n",
    "    \"You have a good place\",\n",
    "    \"I must be ill!\"\n",
    "]\n",
    "# 効率化のため，`LanguageCF`には，テキストに対する値をキャッシュする機能があります．\n",
    "print(\"#Cache:\", len(lcf.cache))\n",
    "tic()\n",
    "h_list = lcf.get_for_texts(test_texts)\n",
    "toc()\n",
    "print(\"#Cache:\", len(lcf.cache))\n",
    "print(h_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@no_grad()\n",
    "def generate(\n",
    "        x0: Tensor,\n",
    "        max_new_tokens: int,\n",
    "        temperature: float,\n",
    "        filter: Filter,\n",
    "        normalizer: Normalizer,\n",
    "        stream: bool = True\n",
    "):\n",
    "    R = {\"disallowed_tokens_history\": [], \"clf_mapping_history\": []}\n",
    "    x = x0.clone()\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        output = Gm(x[None])\n",
    "        logit = output.logits[0][-1]\n",
    "        P = distributionify(logit, temperature=temperature)\n",
    "        filter_result = filter.scan(x, P)\n",
    "        R[\"disallowed_tokens_history\"].append(filter_result.disallowed)\n",
    "        R[\"clf_mapping_history\"].append(filter_result.clf_mapping)\n",
    "        Q = normalizer(P, filter_result.allowed)\n",
    "        iast = Q.multinomial(num_samples=1)\n",
    "        if iast == Gt.eos_token_id:\n",
    "            break\n",
    "        x = hstack((x, iast))\n",
    "        if stream:\n",
    "            streamer.put(iast)\n",
    "\n",
    "    if stream:\n",
    "        streamer.end()\n",
    "\n",
    "    R[\"xf\"] = x\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0str = \"Everyone says you will be a good researcher in the future, but\"\n",
    "x0 = Gtokenize(x0str)\n",
    "h0 = lcf.get_for_text(x0str)\n",
    "print(f\"{x0=}\")\n",
    "print(f\"{h0=}\")\n",
    "\n",
    "TOPK = 30\n",
    "TEMPERATURE = 1\n",
    "normalizer = MinJSDNormalizer()\n",
    "MAX_NEW_TOKENS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NoControl(Llama 3 Output)\n",
    "R = generate(\n",
    "    x0=x0,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    temperature=TEMPERATURE,\n",
    "    # `JustTopkFilter`を指定することでTop-Kのみ行う，すなわち，トークンの取捨選択を行わないフィルタとなる．\n",
    "    filter=JustTopkFilter(\n",
    "        top_k=TOPK\n",
    "    ),\n",
    "    normalizer=normalizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBF(alpha=0.3)\n",
    "R = generate(\n",
    "    x0=x0,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    temperature=TEMPERATURE,\n",
    "    # `CBFFilter`を指定することでCBFフィルタを使用できる．\n",
    "    filter=CBFFilter(\n",
    "        top_k=TOPK,\n",
    "        alpha=0.3,\n",
    "        tokenizer=Gt,\n",
    "        lcf=lcf  # CBFフィルタは制約言語関数の機能を使って駆動する．\n",
    "    ),\n",
    "    normalizer=normalizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonVenv_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
